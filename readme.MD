# ğŸ¤Ÿ BSL Hand Gesture Recognition System

A deep learning system that classifies **British Sign Language (BSL)** hand gestures with **95.74% accuracy** across 34 classes â€” numbers (0â€“10) and alphabet (excluding H, J, Y).

ğŸŒ **Live Demo:** [bsl-handgesturerecognition.streamlit.app](https://bsl-handgesturerecognition-rhu9sdhz9v9pzvxufdw2w7.streamlit.app)

---

## ğŸ“Œ Overview

| Detail | Info |
|---|---|
| Dataset | 34,000 images Â· 34 classes Â· 1,000 per class |
| Model | MobileNetV2 (Transfer Learning + Fine-tuning) |
| Accuracy | 95.74% |
| F1-Score | 95.78% |
| Deployment | Streamlit Web App |

---

## ğŸ—‚ï¸ Dataset

**BSL Hand Gesture Dataset** by Eren Tatepe  
ğŸ“¦ [Kaggle Dataset](https://www.kaggle.com/datasets/drmabroukaabuhmida/bsl-dataset-credits-to-eren-tatepe)

- 34,000 images across 34 BSL gesture classes
- Numbers: Zero (0) to Ten (10)
- Alphabet: Aâ€“Z excluding H, J, Y
- Perfectly balanced â€” 1,000 images per class

---

## ğŸ”§ Pipeline

```
Raw Image
    â”‚
    â–¼
CLAHE Normalisation (LAB L-channel, clipLimit=2.0)
    â”‚
    â–¼
MediaPipe Hand Landmarker (21 keypoints)
    â”‚
    â–¼
Bounding Box + 20% Padding â†’ Crop
    â”‚
    â–¼
Resize to 224Ã—224 â†’ Normalise [0,1]
    â”‚
    â–¼
MobileNetV2 â†’ Softmax (34 classes)
```

---

## ğŸ§  Model Architecture

- **Base:** MobileNetV2 pre-trained on ImageNet (frozen in Phase 1)
- **Head:** GlobalAveragePooling2D â†’ BatchNorm â†’ Dense(256) â†’ Dropout(0.4) â†’ Dense(128) â†’ Dropout(0.3) â†’ Dense(34, Softmax)
- **Phase 1:** Train head only Â· Adam lr=1e-3 Â· 10 epochs Â· Best val acc: 82.87%
- **Phase 2:** Unfreeze last 30 layers Â· Adam lr=1e-4 Â· 20 epochs Â· Best val acc: **95.74%**

---

## ğŸ“Š Results

| Metric | Value |
|---|---|
| Accuracy | 95.74% |
| Precision | 96.04% |
| Recall | 95.74% |
| F1-Score | 95.78% |
| Classes â‰¥ 90% Recall | 30 / 34 |

---

## ğŸš€ Run Locally

**1. Clone the repo**
```bash
git clone https://github.com/yoursmaddyy/bsl-gesture-recognition.git
cd bsl-gesture-recognition
```

**2. Install dependencies**
```bash
pip install -r requirements.txt
```

**3. Add model files**

Download and place in the root directory:
- `mobilenetv2_bsl_final.h5` â€” trained model weights
- `class_indices.json` â€” class index mapping

**4. Run the app**
```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
bsl-gesture-recognition/
â”œâ”€â”€ app.py                  # Streamlit application
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ class_indices.json      # Class index mapping
â”œâ”€â”€ hand_landmarker.task    # MediaPipe model (auto-downloaded)
â”œâ”€â”€ notebook.ipynb          # Training notebook
â””â”€â”€ README.md
```

---

## ğŸ“¦ Requirements

```
streamlit
tensorflow
opencv-python
mediapipe
numpy
Pillow
scikit-learn
```

---

## âš ï¸ Notes

- MediaPipe `hand_landmarker.task` is automatically downloaded at first run.
- GPU recommended for training (Tesla T4 used via Kaggle).

---

## ğŸ™ Credits

- Dataset: [Eren Tatepe via Kaggle](https://www.kaggle.com/datasets/drmabroukaabuhmida/bsl-dataset-credits-to-eren-tatepe)
- MediaPipe: [Google](https://mediapipe.dev)
- MobileNetV2: [Sandler et al., 2018](https://arxiv.org/abs/1801.04381)

---

